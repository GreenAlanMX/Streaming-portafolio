# Database Migration Benchmarking Protocol

## AI Disclosure

This document was created with artificial intelligence assistance. The AI contribution was calculated using the following formula and parameters:

```
Final AI Assistance % = (0.25 × Time %) + (0.35 × Content %) + (0.25 × Complexity %) + (0.15 × Self-Assessment Score)
```

**Calculation:**
- Time saved by AI: 90%
- Content generated by AI: 100%
- Complexity handled by AI: 90%
- Self-assessment score: 100%

**Final AI Assistance: 96.5%**

The majority of this protocol's content, structure, code examples, and technical specifications were generated using AI assistance, with human oversight for educational context and requirements specification.

**AI: ChatGPT-4o**
**Prompts**
[09/17/2025 15:41]: pasa todo eso a un markdown, y agrega un AI disclosure, en donde especifiques el porcentaje de IA usada para hacer el documento usando la siguiente fórmula:
Final AI Assistance % = (0.25 × Time %) + (0.35 × Content %) + (0.25 × Complexity %) + (0.15 × Self-Assessment Score)
En tiempo ponle el 90%, contenido usa el 100%, complejidad ponle el 90% y en self-assessment 100%

---

## Overview

This protocol provides standardized procedures for benchmarking and testing database migrations from CSV to MySQL and JSON to MongoDB for data engineering students.

## Prerequisites

### Software Requirements
- MySQL 8.0+
- MongoDB 6.0+
- Python 3.8+
- Required Python packages: `pandas`, `mysql-connector-python`, `pymongo`, `psutil`

### Hardware Specifications
- Minimum 8GB RAM
- SSD storage recommended
- Multi-core processor (4+ cores)

## Test Environment Setup

### 1. Database Configuration

**MySQL Setup:**
```sql
CREATE DATABASE benchmark_db;
USE benchmark_db;

CREATE TABLE test_table (
    id INT PRIMARY KEY,
    name VARCHAR(255),
    email VARCHAR(255),
    age INT,
    salary DECIMAL(10,2),
    created_at TIMESTAMP
);
```

**MongoDB Setup:**
```javascript
use benchmark_db
db.createCollection("test_collection")
```

### 2. Test Data Generation

Create datasets of varying sizes:
- Small: 1,000 records
- Medium: 10,000 records
- Large: 100,000 records
- Extra Large: 1,000,000 records

## Benchmarking Procedures

### Phase 1: Data Integrity Testing

**Objective:** Verify complete and accurate data migration

**Steps:**
1. Count source records in CSV/JSON files
2. Execute migration scripts
3. Count destination records in databases
4. Compare counts and sample data validation
5. Document any discrepancies

**Success Criteria:** 100% data integrity with zero data loss

### Phase 2: Performance Benchmarking

**Metrics to Measure:**
- Execution time (seconds)
- Records processed per second (RPS)
- Memory consumption (MB)
- CPU utilization (%)
- Disk I/O operations

**Test Cases:**
1. Sequential single-threaded insertion
2. Batch insertion (1000 records per batch)
3. Concurrent multi-threaded insertion
4. Memory-constrained environment testing

### Phase 3: Scalability Testing

**Procedure:**
1. Run identical migration with different data sizes
2. Plot performance curves
3. Identify performance bottlenecks
4. Document scaling limitations

## Implementation Scripts

### Benchmark Runner Template

```python
import time
import psutil
import mysql.connector
import pymongo
import pandas as pd
import json
from datetime import datetime

class MigrationBenchmark:
    def __init__(self):
        self.results = []
        self.process = psutil.Process()
    
    def measure_performance(self, func, *args):
        # Pre-execution metrics
        start_time = time.time()
        start_memory = self.process.memory_info().rss
        start_cpu_times = self.process.cpu_times()
        
        # Execute migration function
        record_count = func(*args)
        
        # Post-execution metrics
        end_time = time.time()
        end_memory = self.process.memory_info().rss
        end_cpu_times = self.process.cpu_times()
        
        metrics = {
            'records': record_count,
            'execution_time': end_time - start_time,
            'memory_delta': end_memory - start_memory,
            'cpu_user_time': end_cpu_times.user - start_cpu_times.user,
            'records_per_second': record_count / (end_time - start_time)
        }
        
        return metrics
    
    def benchmark_mysql_migration(self, csv_file):
        conn = mysql.connector.connect(
            host='localhost',
            user='root',
            password='your_password',
            database='benchmark_db'
        )
        
        df = pd.read_csv(csv_file)
        cursor = conn.cursor()
        
        # Batch insert implementation
        batch_size = 1000
        for i in range(0, len(df), batch_size):
            batch = df.iloc[i:i+batch_size]
            values = [tuple(row) for row in batch.values]
            placeholders = ','.join(['%s'] * len(df.columns))
            query = f"INSERT INTO test_table VALUES ({placeholders})"
            cursor.executemany(query, values)
        
        conn.commit()
        cursor.close()
        conn.close()
        
        return len(df)
    
    def benchmark_mongodb_migration(self, json_file):
        client = pymongo.MongoClient('localhost', 27017)
        db = client['benchmark_db']
        collection = db['test_collection']
        
        with open(json_file, 'r') as f:
            data = json.load(f)
        
        # Batch insert
        batch_size = 1000
        for i in range(0, len(data), batch_size):
            batch = data[i:i+batch_size]
            collection.insert_many(batch)
        
        client.close()
        return len(data)
```

### Data Generator Script

```python
import pandas as pd
import numpy as np
import json
from datetime import datetime, timedelta
import random

def generate_test_dataset(size, format_type='csv'):
    """Generate test dataset for benchmarking"""
    
    # Generate sample data
    data = {
        'id': range(1, size + 1),
        'name': [f'User_{i}' for i in range(1, size + 1)],
        'email': [f'user{i}@example.com' for i in range(1, size + 1)],
        'age': np.random.randint(18, 80, size),
        'salary': np.round(np.random.uniform(30000, 150000, size), 2),
        'created_at': [
            datetime.now() - timedelta(days=random.randint(0, 365))
            for _ in range(size)
        ]
    }
    
    df = pd.DataFrame(data)
    
    if format_type == 'csv':
        filename = f'test_data_{size}.csv'
        df.to_csv(filename, index=False)
    elif format_type == 'json':
        filename = f'test_data_{size}.json'
        # Convert datetime to string for JSON serialization
        df['created_at'] = df['created_at'].dt.isoformat()
        records = df.to_dict('records')
        with open(filename, 'w') as f:
            json.dump(records, f, indent=2)
    
    return filename

# Generate test datasets
for size in [1000, 10000, 100000]:
    generate_test_dataset(size, 'csv')
    generate_test_dataset(size, 'json')
```

## Testing Procedures

### Unit Tests Framework

```python
import unittest
import mysql.connector
import pymongo
import pandas as pd
import json

class TestMigrationIntegrity(unittest.TestCase):
    
    def setUp(self):
        """Set up test environment"""
        self.mysql_config = {
            'host': 'localhost',
            'user': 'root',
            'password': 'your_password',
            'database': 'benchmark_db'
        }
        self.mongo_client = pymongo.MongoClient('localhost', 27017)
        self.mongo_db = self.mongo_client['benchmark_db']
    
    def count_csv_records(self, csv_file):
        """Count records in CSV file"""
        df = pd.read_csv(csv_file)
        return len(df)
    
    def count_mysql_records(self, table_name='test_table'):
        """Count records in MySQL table"""
        conn = mysql.connector.connect(**self.mysql_config)
        cursor = conn.cursor()
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        count = cursor.fetchone()[0]
        cursor.close()
        conn.close()
        return count
    
    def count_mongodb_records(self, collection_name='test_collection'):
        """Count records in MongoDB collection"""
        collection = self.mongo_db[collection_name]
        return collection.count_documents({})
    
    def test_record_count_accuracy(self):
        """Verify all records migrated successfully"""
        # This test should be run after migration
        source_count = self.count_csv_records('test_data_1000.csv')
        mysql_count = self.count_mysql_records()
        
        self.assertEqual(source_count, mysql_count, 
                        f"MySQL record count mismatch: {source_count} vs {mysql_count}")
        
        # Test MongoDB if JSON data available
        try:
            with open('test_data_1000.json', 'r') as f:
                json_data = json.load(f)
            source_json_count = len(json_data)
            mongodb_count = self.count_mongodb_records()
            
            self.assertEqual(source_json_count, mongodb_count,
                           f"MongoDB record count mismatch: {source_json_count} vs {mongodb_count}")
        except FileNotFoundError:
            self.skipTest("JSON test data not available")
    
    def test_performance_thresholds(self):
        """Verify performance meets minimum requirements"""
        benchmark = MigrationBenchmark()
        
        # Test with small dataset
        mysql_metrics = benchmark.measure_performance(
            benchmark.benchmark_mysql_migration, 
            'test_data_1000.csv'
        )
        
        # Assert minimum performance thresholds
        self.assertGreater(mysql_metrics['records_per_second'], 50,
                          "MySQL insertion rate below threshold")
        self.assertLess(mysql_metrics['execution_time'], 60,
                       "MySQL execution time exceeds threshold")
    
    def test_data_integrity_sample(self):
        """Verify data integrity with sample records"""
        # Read first few records from source
        df = pd.read_csv('test_data_1000.csv').head(5)
        
        # Query same records from MySQL
        conn = mysql.connector.connect(**self.mysql_config)
        cursor = conn.cursor()
        cursor.execute("SELECT * FROM test_table LIMIT 5")
        mysql_records = cursor.fetchall()
        cursor.close()
        conn.close()
        
        # Compare key fields
        self.assertEqual(len(df), len(mysql_records))
        
        for i, (_, row) in enumerate(df.iterrows()):
            mysql_row = mysql_records[i]
            self.assertEqual(row['id'], mysql_row[0])
            self.assertEqual(row['name'], mysql_row[1])
            self.assertEqual(row['email'], mysql_row[2])
    
    def tearDown(self):
        """Clean up test environment"""
        self.mongo_client.close()

if __name__ == '__main__':
    unittest.main()
```

### Performance Monitoring Scripts

```python
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime

class BenchmarkAnalyzer:
    def __init__(self):
        self.results = []
    
    def add_result(self, database_type, dataset_size, metrics):
        """Add benchmark result for analysis"""
        result = {
            'database': database_type,
            'dataset_size': dataset_size,
            'execution_time': metrics['execution_time'],
            'records_per_second': metrics['records_per_second'],
            'memory_delta': metrics['memory_delta'],
            'timestamp': datetime.now()
        }
        self.results.append(result)
    
    def generate_performance_report(self):
        """Generate comprehensive performance report"""
        df = pd.DataFrame(self.results)
        
        # Create performance comparison charts
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Execution time comparison
        mysql_data = df[df['database'] == 'MySQL']
        mongo_data = df[df['database'] == 'MongoDB']
        
        axes[0, 0].plot(mysql_data['dataset_size'], mysql_data['execution_time'], 
                       label='MySQL', marker='o')
        axes[0, 0].plot(mongo_data['dataset_size'], mongo_data['execution_time'], 
                       label='MongoDB', marker='s')
        axes[0, 0].set_xlabel('Dataset Size')
        axes[0, 0].set_ylabel('Execution Time (seconds)')
        axes[0, 0].set_title('Execution Time Comparison')
        axes[0, 0].legend()
        axes[0, 0].set_xscale('log')
        
        # Records per second comparison
        axes[0, 1].plot(mysql_data['dataset_size'], mysql_data['records_per_second'], 
                       label='MySQL', marker='o')
        axes[0, 1].plot(mongo_data['dataset_size'], mongo_data['records_per_second'], 
                       label='MongoDB', marker='s')
        axes[0, 1].set_xlabel('Dataset Size')
        axes[0, 1].set_ylabel('Records per Second')
        axes[0, 1].set_title('Throughput Comparison')
        axes[0, 1].legend()
        axes[0, 1].set_xscale('log')
        
        # Memory usage comparison
        axes[1, 0].plot(mysql_data['dataset_size'], mysql_data['memory_delta'] / 1024 / 1024, 
                       label='MySQL', marker='o')
        axes[1, 0].plot(mongo_data['dataset_size'], mongo_data['memory_delta'] / 1024 / 1024, 
                       label='MongoDB', marker='s')
        axes[1, 0].set_xlabel('Dataset Size')
        axes[1, 0].set_ylabel('Memory Usage (MB)')
        axes[1, 0].set_title('Memory Consumption Comparison')
        axes[1, 0].legend()
        axes[1, 0].set_xscale('log')
        
        # Scalability analysis
        mysql_efficiency = mysql_data['records_per_second'] / mysql_data['dataset_size'] * 1000
        mongo_efficiency = mongo_data['records_per_second'] / mongo_data['dataset_size'] * 1000
        
        axes[1, 1].plot(mysql_data['dataset_size'], mysql_efficiency, 
                       label='MySQL', marker='o')
        axes[1, 1].plot(mongo_data['dataset_size'], mongo_efficiency, 
                       label='MongoDB', marker='s')
        axes[1, 1].set_xlabel('Dataset Size')
        axes[1, 1].set_ylabel('Efficiency (RPS/1K records)')
        axes[1, 1].set_title('Scalability Analysis')
        axes[1, 1].legend()
        axes[1, 1].set_xscale('log')
        
        plt.tight_layout()
        plt.savefig('benchmark_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        return df
    
    def export_results(self, filename='benchmark_results.csv'):
        """Export results to CSV for further analysis"""
        df = pd.DataFrame(self.results)
        df.to_csv(filename, index=False)
        return filename
```

## Evaluation Criteria

### Performance Benchmarks

**Minimum Acceptable Performance:**
- Small dataset (1K records): < 10 seconds
- Medium dataset (10K records): < 60 seconds
- Large dataset (100K records): < 300 seconds
- Memory usage: < 1GB for 100K records
- CPU efficiency: > 100 records/second

### Quality Metrics

**Data Integrity:** 100% accuracy required
**Error Handling:** Graceful failure recovery
**Resource Management:** Proper connection cleanup
**Documentation:** Complete benchmark report

## Deliverables

### Required Outputs

1. **Migration Scripts**
   - MySQL migration implementation
   - MongoDB migration implementation
   - Error handling and logging

2. **Benchmark Report**
   - Performance comparison table
   - Scalability analysis graphs
   - Resource utilization charts
   - Bottleneck identification

3. **Test Results**
   - Unit test execution results
   - Performance metrics summary
   - Recommendations for optimization

### Report Template

```markdown
# Database Migration Benchmark Report

## Executive Summary
- Dataset size: [X] records
- MySQL migration time: [X] seconds
- MongoDB migration time: [X] seconds
- Winner: [Database] by [X]% performance advantage

## Test Environment
- Hardware specifications
- Software versions
- Configuration settings

## Methodology
- Test procedures followed
- Data generation approach
- Measurement techniques

## Detailed Results

### Performance Metrics Table
| Metric | MySQL | MongoDB | Winner |
|--------|--------|---------|---------|
| Execution Time (s) | X.XX | X.XX | [DB] |
| Records/Second | XXXX | XXXX | [DB] |
| Memory Usage (MB) | XXX | XXX | [DB] |
| CPU Utilization (%) | XX | XX | [DB] |

### Scalability Analysis
[Include performance curves and scaling behavior]

### Resource Utilization
[Memory and CPU usage patterns]

## Analysis

### Performance Bottlenecks
- Identified bottlenecks for each database
- Impact on overall performance
- Mitigation strategies

### Optimization Recommendations
- Configuration tuning suggestions
- Code improvements
- Hardware recommendations

## Conclusion
[Summary of findings and database selection rationale based on specific use cases]

## Appendix
- Raw benchmark data
- Error logs
- Configuration files
```

## Grading Rubric

| Component | Weight | Criteria |
|-----------|---------|----------|
| Code Implementation | 30% | Functionality, error handling, efficiency |
| Performance Results | 25% | Benchmark accuracy, comprehensive testing |
| Data Integrity | 20% | 100% successful migration, validation |
| Analysis Quality | 15% | Insightful bottleneck identification |
| Documentation | 10% | Complete, clear, professional report |

### Detailed Scoring Guidelines

**Code Implementation (30 points)**
- Functional migration scripts (15 pts)
- Proper error handling (5 pts)
- Code efficiency and optimization (5 pts)
- Code documentation and comments (5 pts)

**Performance Results (25 points)**
- Accurate benchmark measurements (10 pts)
- Comprehensive test coverage (8 pts)
- Statistical analysis validity (7 pts)

**Data Integrity (20 points)**
- 100% record migration accuracy (15 pts)
- Data type preservation (3 pts)
- Validation test implementation (2 pts)

**Analysis Quality (15 points)**
- Bottleneck identification (8 pts)
- Optimization recommendations (4 pts)
- Scalability insights (3 pts)

**Documentation (10 points)**
- Report completeness (5 pts)
- Professional presentation (3 pts)
- Clear conclusions (2 pts)

## Common Issues and Troubleshooting

### Performance Bottlenecks
- Single-row insertions instead of batch operations
- Missing database indexes
- Inadequate connection pooling
- Memory constraints

### Data Integrity Issues
- Character encoding problems
- Date format inconsistencies
- Null value handling
- Primary key conflicts

### Resource Management
- Unclosed database connections
- Memory leaks in large datasets
- Inefficient query patterns
- Concurrent access conflicts

### MySQL-Specific Issues
```sql
-- Common fixes for MySQL performance
SET GLOBAL innodb_buffer_pool_size = 1GB;
SET GLOBAL max_allowed_packet = 64MB;
SET GLOBAL bulk_insert_buffer_size = 256MB;

-- Disable autocommit for batch operations
SET autocommit = 0;
-- Your batch inserts here
COMMIT;
SET autocommit = 1;
```

### MongoDB-Specific Issues
```javascript
// Optimize MongoDB for bulk operations
db.test_collection.createIndex({id: 1});

// Use unordered bulk operations for better performance
var bulk = db.test_collection.initializeUnorderedBulkOp();
// Add operations to bulk
bulk.execute();
```

## Safety and Best Practices

### Data Safety
- Always backup databases before testing
- Use separate test environments
- Implement transaction rollback mechanisms
- Validate data before and after migration

### System Resources
- Monitor system resources during testing
- Set reasonable memory limits
- Implement connection timeouts
- Use connection pooling appropriately

### Security Considerations
- Use secure database credentials
- Implement proper access controls
- Sanitize input data
- Log security-relevant events