{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d742b496",
   "metadata": {},
   "source": [
    "# Phase 2 — Statistical Analysis\n",
    "\n",
    "**Activity 2.1 & 2.2 (Descriptive + Inferential)**\n",
    "\n",
    "Notebook preparado para entregar la Fase 2. Incluye: EDA, medidas de tendencia, detección de outliers, análisis de distribución, data quality report, pruebas de hipótesis, correlaciones, clustering, modelos predictivos y análisis de series temporales.\n",
    "\n",
    "**Archivos esperados**: `content.json`, `users.csv`, `viewing_sessions.csv` en `/mnt/data`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced5be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports y configuración\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,5)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "DATA_PATH = '/mnt/data'\n",
    "content_path = os.path.join(DATA_PATH, 'content.json')\n",
    "users_path = os.path.join(DATA_PATH, 'users.csv')\n",
    "sessions_path = os.path.join(DATA_PATH, 'viewing_sessions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fae892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos\n",
    "# content.json: puede contener 'movies' y 'series' o una lista de objetos\n",
    "with open(content_path, 'r', encoding='utf-8') as f:\n",
    "    content = json.load(f)\n",
    "\n",
    "movies = pd.DataFrame(content.get('movies', []))\n",
    "series = pd.DataFrame(content.get('series', []))\n",
    "if not movies.empty:\n",
    "    movies['content_type'] = 'movie'\n",
    "if not series.empty:\n",
    "    series['content_type'] = 'series'\n",
    "content_df = pd.concat([movies, series], ignore_index=True, sort=False)\n",
    "\n",
    "users_df = pd.read_csv(users_path)\n",
    "sessions_df = pd.read_csv(sessions_path)\n",
    "\n",
    "print('content_df shape:', content_df.shape)\n",
    "print('users_df shape:', users_df.shape)\n",
    "print('sessions_df shape:', sessions_df.shape)\n",
    "\n",
    "# preview\n",
    "display(content_df.head())\n",
    "display(users_df.head())\n",
    "display(sessions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe48aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza y features derivadas\n",
    "# Parseo de timestamps comunes\n",
    "for col in ['start_time','end_time','started_at','ended_at','timestamp']:\n",
    "    if col in sessions_df.columns:\n",
    "        try:\n",
    "            sessions_df[col] = pd.to_datetime(sessions_df[col])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# session_duration_min si hay start & end\n",
    "if ('start_time' in sessions_df.columns) and ('end_time' in sessions_df.columns):\n",
    "    sessions_df['session_duration_min'] = (sessions_df['end_time'] - sessions_df['start_time']).dt.total_seconds() / 60.0\n",
    "\n",
    "# normalizar completion_rate\n",
    "if 'completion_rate' not in sessions_df.columns:\n",
    "    if 'completion_percentage' in sessions_df.columns:\n",
    "        sessions_df['completion_rate'] = sessions_df['completion_percentage'] / 100.0\n",
    "    elif 'percent_watched' in sessions_df.columns:\n",
    "        sessions_df['completion_rate'] = sessions_df['percent_watched'] / 100.0\n",
    "    elif ('watched_seconds' in sessions_df.columns) and ('content_id' in sessions_df.columns):\n",
    "        tmp = content_df.copy()\n",
    "        if 'duration_minutes' in tmp.columns:\n",
    "            tmp['content_duration_min'] = tmp['duration_minutes']\n",
    "        elif 'avg_episode_duration' in tmp.columns:\n",
    "            tmp['content_duration_min'] = tmp['avg_episode_duration']\n",
    "        else:\n",
    "            tmp['content_duration_min'] = np.nan\n",
    "        tmp = tmp[['content_id','content_duration_min']]\n",
    "        sessions_df = sessions_df.merge(tmp, on='content_id', how='left')\n",
    "        sessions_df['completion_rate'] = sessions_df['watched_seconds'] / (sessions_df['content_duration_min']*60)\n",
    "\n",
    "# clip completion_rate to [0,1]\n",
    "if 'completion_rate' in sessions_df.columns:\n",
    "    sessions_df['completion_rate'] = sessions_df['completion_rate'].clip(0,1)\n",
    "\n",
    "print('Columns in sessions_df:', sessions_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f9d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticas descriptivas\n",
    "numeric_cols = sessions_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "desc = sessions_df[numeric_cols].describe().T\n",
    "display(desc)\n",
    "\n",
    "# Central tendency & dispersion for key metrics\n",
    "for m in ['session_duration_min','completion_rate']:\n",
    "    if m in sessions_df.columns:\n",
    "        print('\\nMetric:', m)\n",
    "        print('mean:', sessions_df[m].mean())\n",
    "        print('median:', sessions_df[m].median())\n",
    "        print('std:', sessions_df[m].std())\n",
    "        print('min:', sessions_df[m].min(), 'max:', sessions_df[m].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c72e46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detección de outliers\n",
    "if 'session_duration_min' in sessions_df.columns:\n",
    "    col = 'session_duration_min'\n",
    "    mean = sessions_df[col].mean()\n",
    "    std = sessions_df[col].std()\n",
    "    sessions_df['z_'+col] = (sessions_df[col] - mean) / std\n",
    "    outliers = sessions_df[abs(sessions_df['z_'+col]) > 3]\n",
    "    print('Outliers (z>|3|):', len(outliers))\n",
    "    display(outliers.head(10))\n",
    "\n",
    "    # IQR method\n",
    "    q1 = sessions_df[col].quantile(0.25)\n",
    "    q3 = sessions_df[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5*iqr\n",
    "    upper = q3 + 1.5*iqr\n",
    "    iqr_out = sessions_df[(sessions_df[col] < lower) | (sessions_df[col] > upper)]\n",
    "    print('Outliers by IQR:', len(iqr_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef276ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráficos de distribución y popularidad\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sessions per user\n",
    "if 'user_id' in sessions_df.columns:\n",
    "    sessions_per_user = sessions_df.groupby('user_id').size().rename('sessions_count')\n",
    "    plt.figure()\n",
    "    plt.hist(sessions_per_user.values, bins=40)\n",
    "    plt.title('Distribución de sesiones por usuario')\n",
    "    plt.xlabel('Número de sesiones')\n",
    "    plt.ylabel('Usuarios')\n",
    "    plt.show()\n",
    "\n",
    "# Top content by sessions\n",
    "if 'content_id' in sessions_df.columns:\n",
    "    content_counts = sessions_df['content_id'].value_counts().rename_axis('content_id').reset_index(name='session_views')\n",
    "    if 'content_id' in content_df.columns and 'title' in content_df.columns:\n",
    "        content_counts = content_counts.merge(content_df[['content_id','title']], on='content_id', how='left')\n",
    "    top10 = content_counts.head(10)\n",
    "    plt.figure()\n",
    "    plt.barh(top10['title'].fillna(top10['content_id']), top10['session_views'])\n",
    "    plt.title('Top 10 contenido por sesiones')\n",
    "    plt.xlabel('Sesiones')\n",
    "    plt.show()\n",
    "\n",
    "# Completion rate distribution\n",
    "if 'completion_rate' in sessions_df.columns:\n",
    "    plt.figure()\n",
    "    plt.hist(sessions_df['completion_rate'].dropna(), bins=30)\n",
    "    plt.title('Distribución de completion_rate')\n",
    "    plt.xlabel('Completion rate (0-1)')\n",
    "    plt.show()\n",
    "\n",
    "# Device usage\n",
    "device_col_candidates = ['device','device_type','platform']\n",
    "device_col = next((c for c in device_col_candidates if c in sessions_df.columns), None)\n",
    "if device_col:\n",
    "    device_counts = sessions_df[device_col].value_counts().head(10)\n",
    "    device_counts.plot(kind='bar')\n",
    "    plt.title('Top dispositivos / plataformas')\n",
    "    plt.show()\n",
    "\n",
    "# Geo preferences\n",
    "geo_col_candidates = ['country','country_code','region','location']\n",
    "geo_col = next((c for c in geo_col_candidates if c in sessions_df.columns), None)\n",
    "if geo_col:\n",
    "    geo_counts = sessions_df[geo_col].value_counts().head(10)\n",
    "    geo_counts.plot(kind='bar')\n",
    "    plt.title('Top localidades por sesiones')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343b6ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Report\n",
    "dq = pd.DataFrame({\n",
    "    'n_missing': sessions_df.isna().sum(),\n",
    "    'pct_missing': sessions_df.isna().mean()\n",
    "}).sort_values('pct_missing', ascending=False)\n",
    "display(dq.head(50))\n",
    "\n",
    "print('Duplicated rows in sessions:', sessions_df.duplicated().sum())\n",
    "if 'start_time' in sessions_df.columns:\n",
    "    print('Date range:', sessions_df['start_time'].min(), sessions_df['start_time'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc37f419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis test: Premium vs Basic (session_duration_min)\n",
    "if ('user_id' in sessions_df.columns) and ('user_id' in users_df.columns) and ('subscription_type' in users_df.columns) and ('session_duration_min' in sessions_df.columns):\n",
    "    merged = sessions_df.merge(users_df[['user_id','subscription_type']], on='user_id', how='left')\n",
    "    premium = merged.loc[merged['subscription_type']=='Premium', 'session_duration_min'].dropna()\n",
    "    basic = merged.loc[merged['subscription_type']=='Basic', 'session_duration_min'].dropna()\n",
    "    print('N premium:', len(premium), 'N basic:', len(basic))\n",
    "\n",
    "    levene_stat, levene_p = stats.levene(premium, basic, center='median')\n",
    "    print('Levene p-value:', levene_p)\n",
    "\n",
    "    equal_var = (levene_p > 0.05)\n",
    "    t_stat, p_val = stats.ttest_ind(premium, basic, equal_var=equal_var, nan_policy='omit')\n",
    "    print('t-stat:', t_stat, 'p-value:', p_val)\n",
    "\n",
    "    def cohens_d(a,b):\n",
    "        na, nb = len(a), len(b)\n",
    "        sa, sb = a.std(ddof=1), b.std(ddof=1)\n",
    "        s = np.sqrt(((na-1)*sa*sa + (nb-1)*sb*sb) / (na+nb-2))\n",
    "        return (a.mean()-b.mean())/s\n",
    "    try:\n",
    "        print(\"Cohen's d:\", cohens_d(premium, basic))\n",
    "    except Exception as e:\n",
    "        print('Cohen computation error:', e)\n",
    "else:\n",
    "    print('Faltan columnas para hacer la prueba.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f017b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis: age, session_duration_min, completion_rate\n",
    "if ('user_id' in sessions_df.columns) and ('age' in users_df.columns) and ('session_duration_min' in sessions_df.columns):\n",
    "    merged = sessions_df.merge(users_df[['user_id','age']], on='user_id', how='left')\n",
    "    corr_df = merged[['age','session_duration_min','completion_rate']].dropna()\n",
    "    corr_mat = corr_df.corr(method='pearson')\n",
    "    display(corr_mat)\n",
    "\n",
    "    from itertools import combinations\n",
    "    def pearson_pvalue(x,y):\n",
    "        r, p = stats.pearsonr(x,y)\n",
    "        return r, p\n",
    "\n",
    "    for a,b in combinations(corr_df.columns,2):\n",
    "        r,p = pearson_pvalue(corr_df[a], corr_df[b])\n",
    "        print(f'Pearson r({a},{b})={r:.3f}, p={p:.3e}')\n",
    "else:\n",
    "    print('No hay columnas suficientes para correlación.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50927076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering: user-level aggregation and KMeans\n",
    "agg = sessions_df.groupby('user_id').agg(\n",
    "    sessions_count = ('session_id','count'),\n",
    "    avg_duration = ('session_duration_min','mean'),\n",
    "    avg_completion = ('completion_rate','mean')\n",
    ").reset_index()\n",
    "\n",
    "if 'user_id' in users_df.columns and 'age' in users_df.columns:\n",
    "    agg = agg.merge(users_df[['user_id','age']], on='user_id', how='left')\n",
    "\n",
    "features = ['sessions_count','avg_duration','avg_completion','age']\n",
    "X = agg[features].fillna(0)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Elbow plot\n",
    "inertia = []\n",
    "K = range(1,8)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10).fit(X_scaled)\n",
    "    inertia.append(km.inertia_)\n",
    "plt.figure()\n",
    "plt.plot(list(K), inertia, '-o')\n",
    "plt.xlabel('k'); plt.ylabel('Inertia'); plt.title('Elbow method'); plt.show()\n",
    "\n",
    "# Silhouette\n",
    "from sklearn.metrics import silhouette_score\n",
    "for k in range(2,6):\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10).fit(X_scaled)\n",
    "    print('k=',k,'silhouette=', silhouette_score(X_scaled, km.labels_))\n",
    "\n",
    "# Fit k=3 example\n",
    "k = 3\n",
    "km = KMeans(n_clusters=k, random_state=42, n_init=10).fit(X_scaled)\n",
    "agg['cluster_kmeans'] = km.labels_\n",
    "display(agg.groupby('cluster_kmeans').agg({\n",
    "    'sessions_count':'mean','avg_duration':'mean','avg_completion':'mean','age':'mean','user_id':'count'\n",
    "}).rename(columns={'user_id':'n_users'}))\n",
    "\n",
    "# Dendrogram (muestra)\n",
    "sample = X_scaled[:500]\n",
    "Z = linkage(sample, method='ward')\n",
    "plt.figure(figsize=(12,5))\n",
    "dendrogram(Z, truncate_mode='level', p=5)\n",
    "plt.title('Dendrogram (sample)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5878bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive model: simple RandomForest to predict retention\n",
    "# Define retained as sessions_count >= 5 (example)\n",
    "agg['retained'] = (agg['sessions_count'] >= 5).astype(int)\n",
    "features = ['sessions_count','avg_duration','avg_completion','age']\n",
    "X = agg[features].fillna(0)\n",
    "y = agg['retained']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "try:\n",
    "    print('ROC AUC:', roc_auc_score(y_test, y_prob))\n",
    "except Exception as e:\n",
    "    print('ROC AUC error:', e)\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=features).sort_values(ascending=False)\n",
    "display(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee429fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series: sessions per week and seasonal decomposition\n",
    "if 'start_time' in sessions_df.columns:\n",
    "    ts = sessions_df.copy()\n",
    "    ts['date'] = pd.to_datetime(ts['start_time']).dt.date\n",
    "    daily = ts.groupby('date').size().rename('sessions').to_frame()\n",
    "    daily.index = pd.to_datetime(daily.index)\n",
    "    weekly = daily['sessions'].resample('W').sum()\n",
    "    plt.figure()\n",
    "    weekly.plot(title='Sessions per week')\n",
    "    plt.ylabel('Sessions')\n",
    "    plt.show()\n",
    "\n",
    "    # seasonal decomposition\n",
    "    try:\n",
    "        from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "        res = seasonal_decompose(weekly, period=52, model='additive', two_sided=False)\n",
    "        res.plot()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print('Seasonal decomposition error (maybe series too short):', e)\n",
    "else:\n",
    "    print('No start_time column for timeseries.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f76f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar muestra limpia y wrap-up\n",
    "out_path = os.path.join(DATA_PATH, 'sessions_clean_sample.csv')\n",
    "sessions_df.head(2000).to_csv(out_path, index=False)\n",
    "print('Saved sample to:', out_path)\n",
    "\n",
    "# Save notebook summary (data quality and next steps)\n",
    "print('\\n--- Summary of artifacts ---')\n",
    "print('- Descriptive stats, outliers, distribution plots')\n",
    "print('- Hypothesis test results (Premium vs Basic)')\n",
    "print('- Correlation matrix')\n",
    "print('- Clustering assignment and profiles')\n",
    "print('- Predictive model evaluation (classification)')\n",
    "print('- Time series plots')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
